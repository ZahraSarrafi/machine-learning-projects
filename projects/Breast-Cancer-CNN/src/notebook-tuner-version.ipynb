{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f06988fd",
   "metadata": {},
   "source": [
    "# Breast Cancer Detection With Ultrasound Images\n",
    "# Tuner Version\n",
    "\n",
    "Using CNN Deep Neural Network\n",
    "\n",
    "(with keras_tuner to find the best neural network topology)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c74e77",
   "metadata": {},
   "source": [
    "## About the Dataset\n",
    "\n",
    "This dataset consists of ultrasound images related to benign and malignant breast cancers.\n",
    "\n",
    "The images have been augmented by rotation and sharpening to produce sufficient amount of images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa9efe5",
   "metadata": {},
   "source": [
    "## Importing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253b360e",
   "metadata": {},
   "source": [
    "### Determine all the images filepaths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1202845",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def _get_all_filepath_from_folder(folder_filepath: str) -> list[str]:\n",
    "    all_filepath: list[str] = []\n",
    "    for curr_filename in os.listdir(folder_filepath):\n",
    "        full_path = os.path.join(folder_filepath, curr_filename)\n",
    "        if os.path.exists(full_path):\n",
    "            all_filepath.append(full_path)\n",
    "\n",
    "    return all_filepath\n",
    "\n",
    "train_benign = f\"../assets/ultrasound breast classification/train/benign\"\n",
    "train_malignant = f\"../assets/ultrasound breast classification/train/malignant\"\n",
    "test_benign = f\"../assets/ultrasound breast classification/val/benign\"\n",
    "test_malignant = f\"../assets/ultrasound breast classification/val/malignant\"\n",
    "\n",
    "# make a list of all the filepath for train+benign\n",
    "all_train_benign_filepath    = _get_all_filepath_from_folder(train_benign)\n",
    "all_train_malignant_filepath = _get_all_filepath_from_folder(train_malignant)\n",
    "all_test_benign_filepath     = _get_all_filepath_from_folder(test_benign)\n",
    "all_test_malignant_filepath  = _get_all_filepath_from_folder(test_malignant)\n",
    "\n",
    "print(f\"all_train_benign_filepath    {len(all_train_benign_filepath)}\")\n",
    "print(f\"all_train_malignant_filepath {len(all_train_malignant_filepath)}\")\n",
    "print(f\"all_test_benign_filepath     {len(all_test_benign_filepath)}\")\n",
    "print(f\"all_test_malignant_filepath  {len(all_test_malignant_filepath)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff78fe4a",
   "metadata": {},
   "source": [
    "### Build train/test DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fa25d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use \"zip()\" to merge 2 lists into 1 list of a tuple with 2 elements\n",
    "columns_train_benign     = list(zip(all_train_benign_filepath, [0] * len(all_train_benign_filepath)))\n",
    "columns_train_malignant  = list(zip(all_train_malignant_filepath, [1] * len(all_train_malignant_filepath)))\n",
    "columns_test_benign      = list(zip(all_test_benign_filepath, [0] * len(all_test_benign_filepath)))\n",
    "columns_test_malignant   = list(zip(all_test_malignant_filepath, [1] * len(all_test_malignant_filepath)))\n",
    "\n",
    "# merging the benign/malignant lists into 1 list\n",
    "columns_train_all = columns_train_benign + columns_train_malignant\n",
    "columns_test_all = columns_test_benign + columns_test_malignant\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# here we create the train/test dataframes\n",
    "train_df = pd.DataFrame(columns_train_all, columns=['filepath', 'is_malignant'])\n",
    "test_df = pd.DataFrame(columns_test_all, columns=['filepath', 'is_malignant'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7f6f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train_df.info()')\n",
    "print(train_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef2a1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test_df.info()')\n",
    "print(test_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c647b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train_df')\n",
    "print(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa03e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test_df')\n",
    "print(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa3acde",
   "metadata": {},
   "source": [
    "Here we shuffle the Data frames\n",
    "* Otherwise all the benign values are at the start and all the malignant values at the end\n",
    "* Here we use the random_state parameter to keep it deterministic between each launch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b423b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.sample(frac=1, random_state=0).reset_index(drop=True)\n",
    "test_df = test_df.sample(frac=1, random_state=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e645fb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train_df')\n",
    "print(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff4c0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test_df')\n",
    "print(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167734c8",
   "metadata": {},
   "source": [
    "**Conclusion:** the data is now shuffled and can be safely used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486bbd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2aa7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "img_square_size = 128\n",
    "\n",
    "# will map the images filepath with their loaded images data\n",
    "def _get_img_filepath_to_map(all_filepaths: list[str]) -> dict[str, np.ndarray]:\n",
    "\n",
    "  total = len(all_filepaths)\n",
    "\n",
    "  img_filepath_map: dict[str, np.ndarray] = {}\n",
    "\n",
    "  for index, curr_filepath in enumerate(all_filepaths):\n",
    "\n",
    "    # small feedback of how far in the list we currently are\n",
    "    if index > 0 and index % 1000 == 0:\n",
    "      print(f\" -> loading images -> progress: {index}/{total} ({(index / total) * 100.0:.0f}%)\")\n",
    "\n",
    "    # load the image -> https://keras.io/api/data_loading/image/\n",
    "    loaded_img_data = tf.keras.utils.load_img(\n",
    "      curr_filepath,\n",
    "      color_mode=\"rgb\",\n",
    "      target_size=(img_square_size, img_square_size), # <- will resize\n",
    "      interpolation=\"nearest\", # <- no aliasing when resized, not blurry\n",
    "      keep_aspect_ratio=False,\n",
    "    )\n",
    "\n",
    "    # convert the colored image to gray scales\n",
    "    greyed_img_data = tf.image.rgb_to_grayscale(loaded_img_data)\n",
    "\n",
    "    # convert the image to an array that can be passed to a model\n",
    "    matrix_img_data = tf.keras.utils.img_to_array(greyed_img_data)\n",
    "\n",
    "    # save the loaded image data against it's filepath\n",
    "    img_filepath_map[curr_filepath] = matrix_img_data\n",
    "\n",
    "  return img_filepath_map\n",
    "\n",
    "print(f\"loading + mapping the image data against their filepath\")\n",
    "\n",
    "all_filepath_img_data_map: dict[str, np.ndarray] = {}\n",
    "\n",
    "to_load = [\n",
    "  ('all_train_benign', all_train_benign_filepath),\n",
    "  ('all_train_malignant', all_train_malignant_filepath),\n",
    "  ('all_test_benign', all_test_benign_filepath),\n",
    "  ('all_test_malignant', all_test_malignant_filepath)\n",
    "]\n",
    "for list_name, curr_list in to_load:\n",
    "  print(f\"starting list: '{list_name}'\")\n",
    "\n",
    "  new_dict_of_img_data = _get_img_filepath_to_map(curr_list)\n",
    "  print(f\" ---> newly loaded images done: {len(new_dict_of_img_data)}\")\n",
    "\n",
    "  # add the new dict of loaded images to the main dict \n",
    "  all_filepath_img_data_map |= new_dict_of_img_data\n",
    "  print(f\" -----> total loaded images so far: {len(all_filepath_img_data_map)}\")\n",
    "\n",
    "print('All Done!')\n",
    "print('all_filepath_img_data_map  ->', len(all_filepath_img_data_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d562958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some debug (benign)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "size=1.5\n",
    "nrows=3\n",
    "ncols=8\n",
    "\n",
    "plt.subplots(nrows, ncols, figsize=(ncols*size, nrows*size))\n",
    "\n",
    "for ii in range(0, nrows * ncols):\n",
    "  plt.subplot(nrows, ncols, 1 + ii)\n",
    "  plt.imshow(all_filepath_img_data_map[all_train_benign_filepath[ii]])\n",
    "  plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('img-breast-cancer-benign.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e87de87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some debug (malignant)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "size=1.5\n",
    "nrows=3\n",
    "ncols=8\n",
    "\n",
    "plt.subplots(nrows, ncols, figsize=(ncols*size, nrows*size))\n",
    "\n",
    "for ii in range(0, nrows * ncols):\n",
    "  plt.subplot(nrows, ncols, 1 + ii)\n",
    "  plt.imshow(all_filepath_img_data_map[all_train_malignant_filepath[ii]])\n",
    "  plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('img-breast-cancer-malignant.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c89a920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of image data (train)\n",
    "X_train = np.array(list(map(lambda curr_filepath: all_filepath_img_data_map.get(curr_filepath), train_df['filepath'])))\n",
    "print('X_train.shape', X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ae7ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of is_malignant values (train)\n",
    "y_train = np.array(train_df['is_malignant'])\n",
    "print('y_train.shape', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0f9dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of image data (test)\n",
    "X_test = np.array(list(map(lambda curr_filepath: all_filepath_img_data_map.get(curr_filepath), test_df['filepath'])))\n",
    "print('X_test.shape', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d278e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of is_malignant values (test)\n",
    "y_test = np.array(test_df['is_malignant'])\n",
    "print('y_test.shape', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4071f859",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c50fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def my_keras_code(\n",
    "    units_feature_layers: int,\n",
    "    total_feature_layers: int,\n",
    "    activation_feature_layers: str,\n",
    "    units_dense_layers: int,\n",
    "    total_dense_layers: int,\n",
    "    activation_dense_layers: str,\n",
    "    optimizer: str,\n",
    "    epoch: int,\n",
    ") -> float:\n",
    "\n",
    "    # Build model\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Input((img_square_size, img_square_size, 1)))\n",
    "\n",
    "    # add X feature layers\n",
    "    for i in range(0, total_feature_layers):\n",
    "\n",
    "        my_local_units = (i + 1) * units_feature_layers\n",
    "\n",
    "        model.add(tf.keras.layers.Conv2D(my_local_units, kernel_size=(\n",
    "            3, 3), activation=activation_feature_layers, padding='same'))\n",
    "        model.add(tf.keras.layers.LeakyReLU(alpha=0.1))\n",
    "        model.add(tf.keras.layers.MaxPooling2D(\n",
    "            pool_size=(2, 2), padding='same'))\n",
    "\n",
    "    # now switch to one dense layer\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "    # add X dense layers\n",
    "    for i in range(0, total_dense_layers):\n",
    "        model.add(tf.keras.layers.Dense(\n",
    "            units_dense_layers, activation=activation_dense_layers))\n",
    "        model.add(tf.keras.layers.LeakyReLU(alpha=0.1))\n",
    "\n",
    "    # output layer\n",
    "    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    #\n",
    "    #\n",
    "    #\n",
    "\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.binary_crossentropy,\n",
    "        optimizer=optimizer,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "\n",
    "        # here \"patience\" is the same as \"epoch\"\n",
    "        # -> we're just after the 'restore_best_weights' feature\n",
    "        patience=epoch,\n",
    "\n",
    "        restore_best_weights=True,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_test, y_test),\n",
    "        # callbacks=[early_stopping],\n",
    "        callbacks=[early_stopping],\n",
    "        epochs=epoch,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    y_pred = model.predict(X_test, verbose=0)\n",
    "    y_pred = (y_pred > 0.5)\n",
    "\n",
    "    score = accuracy_score(y_test, y_pred)\n",
    "    print(\"accuracy_score\", score)\n",
    "\n",
    "    # Return a single float as the objective value.\n",
    "    return score  # higher is better\n",
    "\n",
    "\n",
    "class MyTuner(keras_tuner.GridSearch):\n",
    "    def run_trial(self, trial, **kwargs):\n",
    "        hp = trial.hyperparameters\n",
    "        return my_keras_code(\n",
    "\n",
    "            units_feature_layers=hp.Int(\"units_feature_layers\", 32, 512, 32),\n",
    "            total_feature_layers=hp.Int(\"total_feature_layers\", 1, 6, 1),\n",
    "            activation_feature_layers=hp.Choice(\n",
    "                \"activation_feature_layers\", [\"linear\", \"sigmoid\", \"relu\", \"tanh\", \"leaky_relu\"]),\n",
    "\n",
    "            units_dense_layers=hp.Int(\"units_dense_layers\", 32, 512, 32),\n",
    "            total_dense_layers=hp.Int(\"total_dense_layers\", 1, 6, 1),\n",
    "            activation_dense_layers=hp.Choice(\n",
    "                \"activation_dense_layers\", [\"linear\", \"sigmoid\", \"relu\", \"tanh\", \"leaky_relu\"]),\n",
    "\n",
    "            optimizer=hp.Choice(\n",
    "                \"optimizer\", [\"adam\", \"adadelta\", \"adamw\", \"adagrad\"]),\n",
    "\n",
    "            epoch=3,\n",
    "        )\n",
    "\n",
    "\n",
    "tuner = MyTuner(\n",
    "    # important: this set it\n",
    "    objective=keras_tuner.Objective(\"val_accuracy\", direction=\"max\"),\n",
    "    max_trials=10000,\n",
    "    executions_per_trial=2,\n",
    "    # overwrite=True,\n",
    "    # directory=f\"{_get_current_folder()}/my_dir\",\n",
    "    # project_name=\"keep_code_separate\",\n",
    ")\n",
    "\n",
    "tuner.search_space_summary()\n",
    "\n",
    "tuner.search()\n",
    "\n",
    "# Retraining the model\n",
    "best_hp = tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "print(\"best_hp\", best_hp.values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
